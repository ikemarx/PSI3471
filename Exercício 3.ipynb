{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845d85ff-34d0-44f6-8770-7c1c50baad24",
   "metadata": {},
   "source": [
    "# Exercício 3 - PSI3471\n",
    "### Exercício feito individualmente\n",
    "#### Henrique Schneider Marx 14578432\n",
    "\n",
    "Começamos importando as bibliotecas necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "775afc1e-341b-4ed5-9892-b33a229bc024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a6d98-3ee8-48df-96da-0f72d9a2be63",
   "metadata": {},
   "source": [
    "Aqui definimos a função de ativação sigmoidal e sua derivada, bem como a função de perda MSE (erro quadrático médio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef5890f-0265-4f98-99ce-082db8810c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0f120a-6349-4da5-ab40-31872ecc4553",
   "metadata": {},
   "source": [
    "A classe `NeuralNetwork` é definida abaixo. Ela contém a inicialização dos pesos, o método de alimentação para frente (feedforward) e o método de treinamento (train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "634cbfb6-e8d8-47b8-8572-175fd9d6a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def _init_(self, num_inputs, num_hidden, num_outputs):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights_hidden = np.random.randn(num_inputs, num_hidden)\n",
    "        self.bias_hidden = np.zeros(num_hidden)\n",
    "        self.weights_output = np.random.randn(num_hidden, num_outputs)\n",
    "        self.bias_output = np.zeros(num_outputs)\n",
    "        # Initialize parameters for Adam optimizer\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.m_w_hidden = np.zeros_like(self.weights_hidden)\n",
    "        self.v_w_hidden = np.zeros_like(self.weights_hidden)\n",
    "        self.m_b_hidden = np.zeros_like(self.bias_hidden)\n",
    "        self.v_b_hidden = np.zeros_like(self.bias_hidden)\n",
    "        self.m_w_output = np.zeros_like(self.weights_output)\n",
    "        self.v_w_output = np.zeros_like(self.weights_output)\n",
    "        self.m_b_output = np.zeros_like(self.bias_output)\n",
    "        self.v_b_output = np.zeros_like(self.bias_output)\n",
    "        pass\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        hidden = sigmoid(np.dot(x, self.weights_hidden) + self.bias_hidden)\n",
    "        output = sigmoid(np.dot(hidden, self.weights_output) + self.bias_output)\n",
    "        return output\n",
    "        pass\n",
    "\n",
    "    def train(self, data, all_y_trues, learn_rate, epochs):\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                # Forward pass\n",
    "                hidden = sigmoid(np.dot(x, self.weights_hidden) + self.bias_hidden)\n",
    "                output = sigmoid(np.dot(hidden, self.weights_output) + self.bias_output)\n",
    "                y_pred = output\n",
    "\n",
    "                # Calculate partial derivatives\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # Output layer\n",
    "                d_ypred_d_w_output = hidden * deriv_sigmoid(output)\n",
    "                d_ypred_d_b_output = deriv_sigmoid(output)\n",
    "                d_ypred_d_hidden = self.weights_output * deriv_sigmoid(output)\n",
    "\n",
    "                # Update output layer weights and biases using Adam optimizer\n",
    "                # Compute gradients for weights and biases\n",
    "                grad_w_output = d_ypred_d_w_output\n",
    "                grad_b_output = d_ypred_d_b_output\n",
    "\n",
    "                # Update moment estimates for weights\n",
    "                self.m_w_output = self.beta1 * self.m_w_output + (1 - self.beta1) * grad_w_output\n",
    "                self.v_w_output = self.beta2 * self.v_w_output + (1 - self.beta2) * (grad_w_output ** 2)\n",
    "\n",
    "                # Compute bias-corrected moment estimates for weights\n",
    "                m_w_output_hat = self.m_w_output / (1 - self.beta1 ** (epoch + 1))\n",
    "                v_w_output_hat = self.v_w_output / (1 - self.beta2 ** (epoch + 1))\n",
    "\n",
    "                # Update weights\n",
    "                self.weights_output -= learn_rate * m_w_output_hat / (np.sqrt(v_w_output_hat) + self.epsilon)\n",
    "\n",
    "                # Update moment estimates for biases\n",
    "                self.m_b_output = self.beta1 * self.m_b_output + (1 - self.beta1) * grad_b_output\n",
    "                self.v_b_output = self.beta2 * self.v_b_output + (1 - self.beta2) * (grad_b_output ** 2)\n",
    "\n",
    "                # Compute bias-corrected moment estimates for biases\n",
    "                m_b_output_hat = self.m_b_output / (1 - self.beta1 ** (epoch + 1))\n",
    "                v_b_output_hat = self.v_b_output / (1 - self.beta2 ** (epoch + 1))\n",
    "\n",
    "                # Update biases\n",
    "                self.bias_output -= learn_rate * m_b_output_hat / (np.sqrt(v_b_output_hat) + self.epsilon)\n",
    "\n",
    "                # Hidden layer\n",
    "                d_hidden_d_w_hidden = x * deriv_sigmoid(hidden).reshape(self.num_hidden, 1)\n",
    "\n",
    "                # Update hidden layer weights and biases using Adam optimizer\n",
    "                # Compute gradients for weights and biases\n",
    "                grad_w_hidden = d_hidden_d_w_hidden\n",
    "                grad_b_hidden = deriv_sigmoid(hidden)\n",
    "\n",
    "                # Update moment estimates for weights\n",
    "                self.m_w_hidden = self.beta1 * self.m_w_hidden + (1 - self.beta1) * grad_w_hidden\n",
    "                self.v_w_hidden = self.beta2 * self.v_w_hidden + (1 - self.beta2) * (grad_w_hidden ** 2)\n",
    "\n",
    "                # Compute bias-corrected moment estimates for weights\n",
    "                m_w_hidden_hat = self.m_w_hidden / (1 - self.beta1 ** (epoch + 1))\n",
    "                v_w_hidden_hat = self.v_w_hidden / (1 - self.beta2 ** (epoch + 1))\n",
    "\n",
    "                # Update weights\n",
    "                self.weights_hidden -= learn_rate * m_w_hidden_hat / (np.sqrt(v_w_hidden_hat) + self.epsilon)\n",
    "\n",
    "                # Update moment estimates for biases\n",
    "                self.m_b_hidden = self.beta1 * self.m_b_hidden + (1 - self.beta1) * grad_b_hidden\n",
    "                self.v_b_hidden = self.beta2 * self.v_b_hidden + (1 - self.beta2) * (grad_b_hidden ** 2)\n",
    "\n",
    "                # Compute bias-corrected moment estimates for biases\n",
    "                m_b_hidden_hat = self.m_b_hidden / (1 - self.beta1 ** (epoch + 1))\n",
    "                v_b_hidden_hat = self.v_b_hidden / (1 - self.beta2 ** (epoch + 1))\n",
    "\n",
    "                # Update biases\n",
    "                self.bias_hidden -= learn_rate * m_b_hidden_hat / (np.sqrt(v_b_hidden_hat) + self.epsilon)\n",
    "\n",
    "            # Calculate loss for each epoch\n",
    "            y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "            loss = mse_loss(all_y_trues, y_preds)\n",
    "            loss_history.append(loss)\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "        pass\n",
    "\n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a68e2-9f53-496b-afb4-e92a77fb48a1",
   "metadata": {},
   "source": [
    "Definimos as funções matemáticas que queremos que nossa rede neural aprenda a modelar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a868fe-42b5-420a-a1d0-ccdb0ae266f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return 1 / x\n",
    "\n",
    "def f2(x):\n",
    "    return np.log10(x)\n",
    "\n",
    "def f3(x):\n",
    "    return np.exp(-x)\n",
    "\n",
    "def f4(x):\n",
    "    return np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a610bf2-ebfc-4ebe-bb35-6e886ab093e4",
   "metadata": {},
   "source": [
    "Preparamos os conjuntos de dados de treinamento e teste para cada função matemática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90bf034c-12f6-49d6-a9b5-802c6872a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.linspace(1, 100, 10000).reshape(-1, 1)\n",
    "y_train_f1 = f1(x_train)\n",
    "y_train_f2 = f2(x_train[:1000])\n",
    "y_train_f3 = f3(x_train[:1000])\n",
    "y_train_f4 = f4(np.linspace(0, np.pi/2, 1000)).reshape(-1, 1)\n",
    "\n",
    "x_test = np.linspace(1, 100, 1000).reshape(-1, 1)\n",
    "y_test_f1 = f1(x_test)\n",
    "y_test_f2 = f2(x_test[:100])\n",
    "y_test_f3 = f3(x_test[:100])\n",
    "y_test_f4 = f4(np.linspace(0, np.pi/2, 100)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd090bff-a592-43ec-8561-f19b211db304",
   "metadata": {},
   "source": [
    "Aqui, treinamos a rede neural com diferentes números de neurônios na camada oculta e avaliamos seu desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e23538-111b-4ee5-b8b1-b26749110244",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NeuralNetwork() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m results[neurons] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, y_train, y_test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m], [y_train_f1, y_train_f2, y_train_f3, y_train_f4], [y_test_f1, y_test_f2, y_test_f3, y_test_f4]):\n\u001b[0;32m----> 7\u001b[0m     network \u001b[38;5;241m=\u001b[39m NeuralNetwork(num_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_hidden\u001b[38;5;241m=\u001b[39mneurons, num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m     loss_history \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mtrain(x_train, y_train, learn_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      9\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mfeedforward(x_test)\n",
      "\u001b[0;31mTypeError\u001b[0m: NeuralNetwork() takes no arguments"
     ]
    }
   ],
   "source": [
    "hidden_neurons = [3, 4, 5, 10, 15, 20, 50, 100]\n",
    "\n",
    "results = {}\n",
    "for neurons in hidden_neurons:\n",
    "    results[neurons] = {}\n",
    "    for func, y_train, y_test in zip(['f1', 'f2', 'f3', 'f4'], [y_train_f1, y_train_f2, y_train_f3, y_train_f4], [y_test_f1, y_test_f2, y_test_f3, y_test_f4]):\n",
    "        network = NeuralNetwork(num_inputs=1, num_hidden=neurons, num_outputs=1)\n",
    "        loss_history = network.train(x_train, y_train, learn_rate=0.1, epochs=1000)\n",
    "        y_pred = network.feedforward(x_test)\n",
    "        mse = mse_loss(y_test, y_pred)\n",
    "        results[neurons][func] = mse\n",
    "        plt.plot(x_test, y_test, label='Real')\n",
    "        plt.plot(x_test, y_pred, label='NN Prediction')\n",
    "        plt.title(f'Function {func} - Hidden Neurons: {neurons}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('f(x)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d85def-d02c-4d33-96c1-d72804384573",
   "metadata": {},
   "source": [
    "Finalmente, apresentamos os resultados do treinamento, incluindo gráficos das aproximações da rede neural e uma tabela com os valores de MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe7da09-5a07-4339-9fb3-593aed7f783a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Neurons: 3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'f1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHidden Neurons: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneurons\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[neurons][func]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'f1'"
     ]
    }
   ],
   "source": [
    "for neurons in hidden_neurons:\n",
    "    print(f'Hidden Neurons: {neurons}')\n",
    "    for func in ['f1', 'f2', 'f3', 'f4']:\n",
    "        print(f'Function {func} MSE: {results[neurons][func]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e15110-ecd4-4485-9362-ba508022bbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
